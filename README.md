# Volumetric Clouds AI

A student project implementing neural super-resolution for volumetric cloud rendering using a U-Net architecture. The project consists of two main components: a C++ OpenGL rendering application that generates training data, and a PyTorch-based training pipeline.

**Note**: The rendering runtime (`Source/Runtime`) is adapted from a previous [Graphics Programming course project at ITU](https://github.com/gabrieldelacruz/graphics-programming-2023), modified for this AI-focused work.

## Requirements

### C++ Rendering Application
- **OS**: Windows only
- **IDE**: Visual Studio (tested with VS 2022)
- **Graphics**: OpenGL (Intel GPU required - NVIDIA GPUs may have compatibility issues)
- **Solution file**: `VolumetricCloudsAI.sln`

### Python Training Pipeline
- **Python**: 3.10 (3.11+ not supported)
- **Package manager**: [uv](https://github.com/astral-sh/uv) (required)
- **GPU**: CUDA 12.8 compatible GPU recommended for training (tested on RTX 50 series)

## Getting Started

### 1. Generate Training Data (C++ Application)

1. Open `VolumetricCloudsAI.sln` in Visual Studio
2. Build and run the solution
3. A window opens with a volumetric cloud scene:
   - Use **WASD** to navigate the camera
   - Open the **"Training Capture"** GUI panel
   - Configure capture settings (target pairs, resolution scale, etc.)
   - Click **"Start Training Capture"** to begin generating data
   - The application outputs to `TrainingCaptures/` in the working directory (default: 1024 pairs)
4. **Note**: Training captures are NOT included in this repository due to size (~60 GB)

**Optional command-line arguments:**
```bash
VolumetricCloudsRender.exe --training --batch-size=1024 --training-output=./MyCaptures
```

The C++ source code is located in:
- `Source/VolumetricCloudsRender` - Main rendering application
- `Source/Runtime` - Runtime/engine code

### 2. Set Up Python Environment

Navigate to the training directory:

```bash
cd Source/VolumetricCloudsTraining
```

Install dependencies:

```bash
uv sync
```

### 3. Copy Training Data

Copy the `TrainingCaptures` folder generated by the C++ application into the `Source/VolumetricCloudsTraining` directory.

```
Source/VolumetricCloudsTraining/
├── TrainingCaptures/        # <- Copy here
├── main.py
├── config/
└── ...
```

### 4. Train the Model

Train a model using one of the provided configuration files:

```bash
uv run python main.py --mode train --config .\config\config_exp1_buffers_singleview.yaml
```

Available configurations:
- `config_exp0_overfit_singleview.yaml` - Overfitting test on small dataset
- `config_exp0_overfit_general.yaml` - Overfitting test (general views)
- `config_exp1_buffers_singleview.yaml` - Buffer ablation study (single view)
- `config_exp1_buffers_general.yaml` - Buffer ablation study (general views)
- `config_exp2_data_efficiency_singleview.yaml` - Data efficiency experiments (single view)
- `config_exp2_data_efficiency_general.yaml` - Data efficiency experiments (general views)
- `config.yaml` - Default configuration

**Training outputs** are saved to `outputs/<experiment_name>/`:
- `checkpoints/` - Model weights saved per epoch (`.pt` files)
- `logs/` - Training logs including:
  - `loss_curve.txt` - Per-step and per-epoch loss values
  - `ssim_curve.txt` - Validation SSIM per epoch
  - `loss_curve.png` - Loss visualization
- Configuration snapshot (copy of the YAML used)

For multi-experiment configs, outputs are organized as:
```
outputs/<experiment_name>/experiments/
├── experiment_1/
│   ├── checkpoints/
│   └── logs/
├── experiment_2/
│   └── ...
└── result.png  # Aggregated comparison plot
```

### 5. Run Inference

Run inference on trained models:

```bash
uv run python main.py --mode infer --config .\config\config_exp1_buffers_singleview.yaml
```

The inference configuration in the YAML file controls:
- Which checkpoint to use
- Input data location
- Output directory for predictions
- Model architecture parameters (must match training)

**Inference outputs** are high-resolution predictions saved as PFM files with a configurable suffix (default: `_pred.pfm`). For example, if the input is `frame_0001_low.pfm`, the output will be `frame_0001_pred.pfm`.

## Additional Utilities

The project includes a few utility scripts used for report generation:
- `analyze_experiments.py` - Summarizes metrics from all experiments
- `plot_experiments.py` - Generates loss/SSIM plots for the report
- `make_paper_qual_figures.py` - Creates qualitative comparison figures

## Project Structure

```
.
├── VolumetricCloudsAI.sln          # Visual Studio solution
├── Source/
│   ├── VolumetricCloudsRender/     # C++ rendering application
│   ├── Runtime/                    # Engine runtime code
│   └── VolumetricCloudsTraining/   # Python training pipeline
│       ├── main.py                 # Entry point for training/inference
│       ├── analyze_experiments.py  # Experiment analysis tool
│       ├── plot_experiments.py     # Plotting utilities
│       ├── make_paper_qual_figures.py  # Generate comparison figures
│       ├── config/                 # YAML configuration files
│       ├── src/                    # Python source code
│       │   └── VolumetricCloudsTraining/
│       │       ├── train_unet.py   # Training logic
│       │       ├── infer_unet.py   # Inference logic
│       │       ├── dataset_cloud_pairs.py  # Dataset loader
│       │       └── models/
│       │           └── unet.py     # U-Net architecture
│       └── outputs/                # Training outputs (generated)
└── Report/                         # LaTeX report
```

## Known Limitations

- **Windows-only**: The C++ application uses Windows-specific APIs
- **Intel GPU requirement**: OpenGL shaders are optimized for Intel GPUs; NVIDIA GPUs may encounter issues
- **Python 3.10 required**: PyTorch dependencies are locked to Python 3.10
- **Training data not included**: The `TrainingCaptures` directory must be generated locally

## Configuration

Edit YAML files in the `config/` directory to customize:
- Dataset paths and splits
- Model architecture (channels, bilinear upsampling, residual learning)
- Training hyperparameters (epochs, batch size, learning rate)
- Feature toggles (view transmittance, light transmittance, depth, normals)
- Experiment configurations for ablation studies
