# ================================================================
# Volumetric Clouds Training Configuration - Default (Single View)
# ================================================================
#
# This file exists as a convenient default for:
#   python main.py --mode train
#
# Paper experiments live in:
# - config_exp0_overfit_singleview.yaml / config_exp0_overfit_general.yaml
# - config_exp1_buffers_singleview.yaml / config_exp1_buffers_general.yaml
# - config_exp2_data_efficiency_singleview.yaml / config_exp2_data_efficiency_general.yaml
#

# Single-view dataset (pre-rendered).
name: default_singleview

common_model: &common_model
  model_base_channels: 32  # Base number of channels in the UNet.
  model_bilinear: true     # Use bilinear upsampling in the decoder.
  model_learn_residual: true  # Predict residual w.r.t. input.

common_split: &common_split
  train_fraction: 0.8   # Fraction of samples for training split.
  val_fraction: 0.15    # Fraction of samples for validation split.
  test_fraction: 0.05   # Fraction of samples for test split.
  split_seed: 12345     # Fixed seed for deterministic split.

common_aux: &common_aux
  use_view_transmittance: false
  use_light_transmittance: false
  use_linear_depth: false
  use_normals: false
  depth_normalization_max: 70000.0  # Depth normalization scale.

train:
  # Merge shared defaults; fields below can override them if needed.
  <<: [*common_model, *common_split, *common_aux]

  data_dir: TrainingCaptures_Single  # Single-view dataset root.
  output_dir: Outputs                # Base output directory.

  # Training schedule ---------------------------------------------------------
  epochs: 100            # Default training length.
  batch_size: 64         # Batch size for training.
  learning_rate: 0.0001  # Learning rate for optimizer.
  crop_size: 256         # Training crop size.
  limit_pairs: null      # Use all pairs.

  # Loss configuration ---------------------------------------------------------
  use_auxiliary_in_loss: false  # Plain RGB L1 loss; auxiliary buffers only affect inputs.

  # Runtime behaviour ---------------------------------------------------------
  num_workers: 8
  device: cuda
  save_epoch_stride: 20
  save_every_n_steps: 8192
  log_every_n_steps: 256
  save_only_last_epoch: false
  export_every_n_epochs: 50

infer:
  # Model checkpoint to use for inference (can be updated as needed).
  checkpoint: Outputs/checkpoints/unet_epoch_100.pt

  # Model configuration ---------------------------------------------------------
  <<: [*common_model, *common_split, *common_aux]

  # Inference input settings ----------------------------------------------------
  input: TrainingCaptures_Single
  input_glob: "*_low.pfm"
  split_mode: test
  split_sample_index: 0
  recursive: false

  # Output directory and naming -------------------------------------------------
  output_dir: Outputs/infer
  output_suffix: "_pred"

  # Runtime behaviour -----------------------------------------------------------
  device: cuda
  upsample_mode: bicubic
  align_corners: false
  clamp_min: 0.0
  clamp_max: 1.0
  scale_factor: 4


